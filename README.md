# GPT-2 Rebuild nanoGPT

Welcome to **GPT-2 Rebuild nanoGPT**!  
This project is a beginner-friendly, step-by-step reimplementation of [@karpathy/build-nanogpt](https://github.com/karpathy/build-nanogpt), inspired by [this fantastic lecture](https://youtu.be/l8pRSuU81PU?feature=shared) by Andrej Karpathy. The goal is to demystify how powerful language models like GPT-2 work by building them from the ground up, with clear explanations and practical code.

---

## üìñ Table of Contents
- [Introduction](#introduction)
- [Original Project Overview](#original-project-overview)
- [Key Features](#key-features)
- [System Requirements](#system-requirements)
- [Installation](#installation)
- [Usage](#usage)
- [Visual Guide](#visual-guide)
- [Troubleshooting](#troubleshooting)
- [Contribution Guidelines](#contribution-guidelines)
- [Acknowledgments](#acknowledgments)
- [License](#license)
- [FAQs](#faqs)
- [Additional Resources](#additional-resources)
- [Community](#community)
- [Contact](#contact)

---

## Introduction

**GPT-2 Rebuild nanoGPT** is designed to help anyone‚Äîfrom beginners to advanced users‚Äîunderstand and experiment with the fundamental concepts behind transformer-based language models. By following this repository, you'll gain hands-on experience in building, training, and using a miniaturized version of GPT-2, with concise, easy-to-read code and helpful documentation.

---

## Original Project Overview

[@karpathy/build-nanogpt](https://github.com/karpathy/build-nanogpt) is an educational repository by Andrej Karpathy that demonstrates how to build a small, efficient version of GPT (Generative Pre-trained Transformer). It focuses on clarity, simplicity, and accessibility, making it ideal for learning and experimentation.

---

## Key Features

- üß† **Educational:** Clean, well-commented code and explanations for every step.
- ‚ö° **Lightweight:** Minimal dependencies and efficient implementation.
- üõ†Ô∏è **Customizable:** Easily experiment with model parameters and datasets.
- üìö **Guided:** Inspired by Karpathy‚Äôs lecture, with code closely following the presented concepts.
- üèóÔ∏è **Extensible:** Use as a foundation for further research or personal projects.

---

## System Requirements

- Python 3.8 or newer
- pip (Python package manager)
- Compatible OS: Windows, macOS, or Linux
- (Optional) CUDA-enabled GPU for faster training

**Recommended Hardware:**  
- At least 4GB RAM  
- (Optional) NVIDIA GPU with at least 4GB VRAM for GPU acceleration

---

## Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/AdilShamim8/GPT-2-Rebuild-nanoGPT.git
   cd GPT-2-Rebuild-nanoGPT
   ```

2. **Create a virtual environment (recommended)**
   ```bash
   python -m venv venv
   source venv/bin/activate   # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

---

## Usage

### 1. **Training the Model**
Train on your own text dataset (e.g., `input.txt`):

```bash
python train.py --data input.txt --epochs 10 --batch_size 64
```

### 2. **Generating Text**
After training, generate new text:

```bash
python generate.py --checkpoint model_checkpoint.pt --prompt "Once upon a time"
```

### 3. **Example Output**
```
Prompt: Once upon a time
Generated: Once upon a time, there was a small language model that could write stories...
```

### 4. **Configuration Options**
- Change model size, layers, or training parameters in `config.py`.
- Try different datasets for unique outputs.

---

## Visual Guide

Below is a simplified diagram of how the transformer model processes text:

```
[Input Text] ‚Üí [Tokenizer] ‚Üí [Embedding Layer] ‚Üí [Transformer Blocks] ‚Üí [Output Layer] ‚Üí [Generated Text]
```

![Transformer Architecture Example](https://raw.githubusercontent.com/karpathy/build-nanogpt/main/docs/transformer-diagram.png)

---

## Troubleshooting

- **ModuleNotFoundError:** Ensure all dependencies are installed with `pip install -r requirements.txt`.
- **CUDA errors:** If you don't have a compatible GPU, set the device to CPU in the config.
- **Memory issues:** Try lowering the batch size or model size in `config.py`.
- **Training is slow:** Use a GPU or reduce the dataset size for quicker iterations.

---

## Contribution Guidelines

We welcome contributions! To get started:

1. Fork the repo and create your branch: `git checkout -b feature/your-feature`
2. Commit your changes: `git commit -am 'Add new feature'`
3. Push to your branch: `git push origin feature/your-feature`
4. Open a Pull Request

See [CONTRIBUTING.md](CONTRIBUTING.md) for more details.

---

## Acknowledgments

- **Special thanks to [@karpathy](https://github.com/karpathy) and [build-nanogpt](https://github.com/karpathy/build-nanogpt)** for the original concept, code, and educational content.
- Inspired by the [YouTube lecture](https://youtu.be/l8pRSuU81PU?feature=shared).

---

## License

This project is licensed under the [MIT License](LICENSE).

---

## FAQs

**Q: Do I need GPU to use this project?**  
A: No, but a GPU is recommended for faster training. It will also work on CPU (just slower).

**Q: Can I use my own dataset?**  
A: Absolutely! Just replace `input.txt` with your text file when training.

**Q: I‚Äôm new to deep learning. Can I follow along?**  
A: Yes! The code and documentation are written for beginners. Check out the resource links below for more background.

---

## Additional Resources

- [@karpathy/build-nanogpt](https://github.com/karpathy/build-nanogpt)
- [Lecture Video](https://youtu.be/l8pRSuU81PU?feature=shared)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

---

## Community

- **Discussions & Support:** [GitHub Discussions](https://github.com/AdilShamim8/GPT-2-Rebuild-nanoGPT/discussions)
- **Chat:** [Gitter](https://gitter.im/)
- **Twitter:** [@karpathy](https://twitter.com/karpathy)

---

## Contact

For questions, suggestions, or feedback, feel free to open an [issue](https://github.com/AdilShamim8/GPT-2-Rebuild-nanoGPT/issues) or contact:

- **Project Maintainer:** [AdilShamim8](https://github.com/AdilShamim8)
- **Email:** adilshamim8@gmail.com

Happy building and learning! üöÄ
